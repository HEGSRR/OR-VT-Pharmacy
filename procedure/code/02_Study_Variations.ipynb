{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Spatio-Temporal Accessibility of Pharmacy Care in Vermont, USA\n",
    "\n",
    "## Alternative Study Versions\n",
    "\n",
    "### Authors\n",
    "\n",
    "- Sam Roubin, sroubin@middlebury.edu, https://orcid.org/0009-0005-5490-3744, Middlebury College\n",
    "- Joseph Holler\\*, josephh@middlebury.edu, https://orcid.org/0000-0002-2381-2699, Middlebury College\n",
    "- Peter Kedron, peterkedron@ucsb.edu, https://orcid.org/0000-0002-1093-3416, University of California, Santa Barbara\n",
    "\n",
    "\\* Corresponding author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Materials and procedure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpUbJuwsgQJu"
   },
   "source": [
    "### Computational environment\n",
    "\n",
    "Similar to Kang et al. (2020), this study was run using CyberGIS-Jupyter. This study uses an updated software environment from the reproduction study, using Python Jupyter Notebooks in the CyberGISX environment available at https://cybergisxhub.cigi.illinois.edu/. In particular, we use the Python 3-0.9.0 Kernel running Python 3.8.12, pandas 1.3.5, geopandas 0.10.2, networkx 2.6.3 and osmnx 1.1.2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import re\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import IPython\n",
    "import requests\n",
    "from IPython.display import display, clear_output\n",
    "from shapely.ops import nearest_points   #for pharmacy_setting function\n",
    "from scipy.stats import kruskal\n",
    "from matplotlib import colors\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.patheffects as path_effects\n",
    "from tabulate import tabulate\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.patches import Patch\n",
    "import imageio\n",
    "import math\n",
    "!pip install scikit-posthocs -q\n",
    "import scikit_posthocs as sp\n",
    "import zipfile\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "if os.path.basename(os.getcwd()) == 'code':\n",
    "    os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Study parameters\n",
    "\n",
    "*Distance*: This study was first conducted using `10`, `20`, and `30` minute distance travel time thresholds.\n",
    "\n",
    "*Friction of distance* is determined by a gaussian function and beta coefficient. The study first used a beta coefficient of `262` and discretized a weight to each distance band according to the `minimum` travel time of the band.\n",
    "\n",
    "*Staffing data source*: Choose to run the study with original confidential data or with a simulated dataset for reproducibility.\n",
    "\n",
    "*Time*: This study was first conducted based on data on 192 pharmacies through `December 2023`.\n",
    "Since that time, three community pharmacies have opened in Vermont and eleven pharmacies have closed in Vermont. \n",
    "This number includes four Rite Aid locations slated for closure or sale later this year.\n",
    "We have not tracked pharmacy openings and closings outside of the state.\n",
    "Below, use a status year of 2023 to generate results as of December 2023 and 2025 to generate results as of `June 2025`.\n",
    "\n",
    "*Pharmacy Technicians*: This study began with a working assumption that one pharmacy technician adds the equivalent level of services to a community pharmacy as that of `pt5` or `0.5` pharmacists. This variable can be adjusted with an alternative FTE.\n",
    "\n",
    "*Demand Population*: This study began studying the total population. It can be varied by studying alternative subsets of the population, e.g. those aged 65 and above.\n",
    "\n",
    "*Geography Level*: The study initially used county subdivisions for population data. Alternatively, change to census `blocks` for a finer-grained analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a unique ID for the set of results\n",
    "setID_file = \"./data/derived/public/result_sets/setID.txt\"\n",
    "\n",
    "with open(setID_file, 'r') as f:\n",
    "    last_id = int(f.read().strip())\n",
    "setID = str(last_id + 1).zfill(3)\n",
    "with open(setID_file, 'w') as f:\n",
    "    f.write(setID)\n",
    "\n",
    "# beta coefficient\n",
    "beta = 262\n",
    "# discretization preference: min or mean\n",
    "discretize = \"min\"\n",
    "# define three distances in minutes in ascending order \n",
    "dist_mins = [10, 20, 30]\n",
    "\n",
    "# calculate distance thresholds in seconds and distance weights\n",
    "distances = [i * 60 for i in dist_mins]\n",
    "\n",
    "def dist_weight(dist, beta):\n",
    "    result = math.exp(-(dist ** 2) / beta)\n",
    "    return round(result, 2)\n",
    "\n",
    "# calculate distance weights\n",
    "if discretize == \"min\":\n",
    "    weights = [1]\n",
    "    for d in dist_mins[:-1]:\n",
    "        weights.append(dist_weight(d, beta))\n",
    "elif discretize == \"mean\":\n",
    "    weights = []\n",
    "    for ind in range(len(dist_mins)):\n",
    "        if ind == 0:\n",
    "            weights.append(1)\n",
    "        else:\n",
    "            dist = (dist_mins[ind-1] + dist_mins[ind]) / 2\n",
    "            weights.append(dist_weight(dist, beta))\n",
    "\n",
    "# set staffing source to \"confidential\" for original survey data \n",
    "# or switch to \"simulated\" for simulated data with the same statistical properties\n",
    "staffing_src = \"confidential\"\n",
    "            \n",
    "# set status year to \"2023\" or \"2025\" for pharmacies as of December 2023 or June 2025\n",
    "# or to 'closed' to vizualize closed pharmacies\n",
    "temporal_extent = \"2023\"\n",
    "\n",
    "# set the techFTE to the pharmacist full-time equivalent value of a pharmacy technician\n",
    "techFTE = 0.5\n",
    "tech_ratio = \"pt\" + str(int(techFTE * 10))\n",
    "\n",
    "# switch from total_pop to other population variables, e.g. elderly_pop\n",
    "demand_population = \"total_pop\"\n",
    "\n",
    "# switch from county_subdivision to block to calculate at the block level\n",
    "geog_level = \"county_subdivision\"\n",
    "\n",
    "# save figures? Switch to True to save figures\n",
    "figsave = False\n",
    "def make_fig_file(fignum):\n",
    "    fig_folder = \"./results/figures/set_\" + setID\n",
    "    os.makedirs(fig_folder, exist_ok=True)\n",
    "    return fig_folder + \"/figure\" + str(fignum) + \".png\"\n",
    "\n",
    "setDesc = \"Result Set ID: \" + setID + \"\\n\" + \\\n",
    "          \"Beta: \" + str(beta) + \"\\n\" + \\\n",
    "          \"Discretization: \" + discretize + \" travel time\" + \"\\n\" + \\\n",
    "          \"Distances: \" + str(distances) + \" (seconds) \" + str(dist_mins) + \" (minutes)\" + \"\\n\" + \\\n",
    "          \"Weights: \" + str(weights) + \"\\n\" + \\\n",
    "          \"Staffing data: \" + staffing_src + \"\\n\" +\\\n",
    "          \"Time series: \" + temporal_extent + \"\\n\" + \\\n",
    "          \"Pharmacy technician value: \" + tech_ratio + \" (\" + str(techFTE) + \")\" + \"\\n\" + \\\n",
    "          \"Demand population: \" + demand_population + \"\\n\" + \\\n",
    "          \"Geographic level of aggregation: \" + geog_level\n",
    "\n",
    "setDesc_file = \"./data/derived/public/result_sets/results_\" + setID + \".txt\"\n",
    "\n",
    "with open(setDesc_file, 'w') as f:\n",
    "    f.write(setDesc)\n",
    "\n",
    "print(setDesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pharmacy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pharmacies = gpd.read_file('./data/raw/public/pharmacy/pharmacies_df.gpkg')\n",
    "if staffing_src == \"confidential\":\n",
    "    pharm_staffing = pd.read_csv('./data/raw/private/staffing_interpolated.csv')\n",
    "else:\n",
    "    pharm_staffing = pd.read_csv('./data/raw/public/pharmacy/pharm_simulated.csv')\n",
    "    pharm_cols = ['pharmid', 'week_sim_staff', 'sat_sim_staff', 'sun_sim_staff']\n",
    "    pharm_staffing = pharm_staffing[pharm_cols]\n",
    "    \n",
    "pharmacies_df = pd.merge(pharmacies, pharm_staffing, on='pharmid', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Population Data\n",
    "\n",
    "First, load county subdivisions and their NECTA classifications.  \n",
    "Second, load census blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in population data by town\n",
    "population_df = gpd.read_file('./data/derived/public/population/county_subdivisions.gpkg')\n",
    "\n",
    "# Read in metropolitan / micropolitan classifications (NECTAS)\n",
    "nectas_df =  gpd.read_file('./data/raw/public/population/nectas.csv')\n",
    "nectas_df['GEOID'] = nectas_df['fips_state'] + nectas_df['fips_county'] + nectas_df['fips_subdivision']\n",
    "\n",
    "# Join NECTAS classifications to population data with subdivision and county FIPS\n",
    "pop_df = pd.merge(population_df, nectas_df[['GEOID', 'necta']], on=['GEOID'], how = 'left')\n",
    "pop_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate population density of each county subdivision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area and population density\n",
    "pop_df.to_crs(\"EPSG:6589\", inplace = True)\n",
    "pop_df['town_area_km2'] = pop_df.geometry.area / 10**6\n",
    "pop_df['pop_density'] = pop_df['total_pop']/pop_df['town_area_km2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize and report the number of county subdivisions per NECTA category for Vermont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'necta' column with 'Rural'. All towns that are not metropolitan or micropolitan are rural. \n",
    "pop_df['necta'].fillna('Rural', inplace=True)\n",
    "\n",
    "# Replace 'Metropolitan NECTA' and 'Micropolitan NECTA' with 'Metropolitan' and 'Micropolitan', respectively\n",
    "pop_df['necta'] = pop_df['necta'].replace({'Metropolitan NECTA': 'Metropolitan', 'Micropolitan NECTA': 'Micropolitan'})\n",
    "\n",
    "# select Vermont subdivisions\n",
    "vermont_pop_df = pop_df[pop_df['GEOID'].str[:2] == '50']\n",
    "#vermont_pop_df = vermont_pop_df.to_crs(\"EPSG:6589\", inplace=True)\n",
    "\n",
    "# summarize counts of NECTA types\n",
    "necta_summary = vermont_pop_df[['necta', 'GEOID']].groupby(\"necta\").count()\n",
    "necta_summary.rename(columns={\"GEOID\": \"N\"}, inplace=True)\n",
    "print(necta_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If analyzing population with census blocks, load and merge blocks for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in population data by block\n",
    "if geog_level == \"block\":\n",
    "    blocks25 = gpd.read_file('./data/derived/public/population/blocks25.gpkg')\n",
    "    blocks33 = gpd.read_file('./data/derived/public/population/blocks33.gpkg')\n",
    "    blocks36 = gpd.read_file('./data/derived/public/population/blocks36.gpkg')\n",
    "    blocks50 = gpd.read_file('./data/derived/public/population/blocks50.gpkg')\n",
    "    blocks_df = pd.concat([blocks25, blocks33, blocks36, blocks50])\n",
    "    blocks_df.plot()\n",
    "    blocks_df.to_crs(\"EPSG:6589\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Study Area\n",
    "\n",
    "#### Mapping town types, population density, and pharmacy locations in VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pharmacies for mapping\n",
    "pharmacies_map = pharmacies_df.to_crs(\"EPSG:6589\") # Match CRS for Vermont\n",
    "pharmacies_map = pharmacies_map[pharmacies_map['state'] == 'VT']\n",
    "\n",
    "# Custom color map for pop density\n",
    "base_cmap = plt.get_cmap('Blues')\n",
    "num_colors = 256\n",
    "\n",
    "custom_colors = []\n",
    "for i in range(num_colors):\n",
    "    color = colors.rgb_to_hsv(base_cmap(i)[:3])  # Convert RGB to HSV\n",
    "    color[1] *= 0.7  # Reduce saturation by one third\n",
    "    custom_colors.append(colors.hsv_to_rgb(color))  # Convert back to RGB\n",
    "\n",
    "custom_cmap = colors.LinearSegmentedColormap.from_list(\"custom_blues\", custom_colors, N=num_colors)\n",
    "\n",
    "# Map Population Density\n",
    "fig1, ax = plt.subplots(figsize=(12.3, 10.8))\n",
    "vermont_pop_df.plot(column='pop_density', scheme = 'jenkscaspallforced',k=5, cmap=custom_cmap, legend = False, ax=ax)\n",
    "\n",
    "metropolitan_df = vermont_pop_df[vermont_pop_df['necta'] == 'Metropolitan']\n",
    "micropolitan_df = vermont_pop_df[vermont_pop_df['necta'] == 'Micropolitan']\n",
    "\n",
    "metropolitan_boundary = metropolitan_df.dissolve(by='necta')['geometry'].boundary\n",
    "micropolitan_boundary = micropolitan_df.dissolve(by='necta')['geometry'].boundary\n",
    "\n",
    "# Plot only the merged exterior boundaries of the 'Metropolitan NECTA' group\n",
    "metropolitan_boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "\n",
    "# Plot only the merged exterior boundaries of the 'Micropolitan NECTA' group\n",
    "micropolitan_boundary.plot(ax=ax, color='black', linestyle = 'dashed', linewidth=1.5)\n",
    "\n",
    "vt_boundary = vermont_pop_df.dissolve().boundary\n",
    "vt_boundary.plot(ax=ax, color='black', linewidth=.75)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='white', markeredgecolor='black', markersize=8, label='Open'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='greenyellow', markeredgecolor='black', markersize=8, label='Newly Opened'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markeredgecolor='black', markersize=8, label='Closed'),\n",
    "    Line2D([0], [0], color='black', lw=2, label='Metropolitan'),\n",
    "    Line2D([0], [0], color='black', lw=2, linestyle= 'dashed', label='Micropolitan')\n",
    "]\n",
    "\n",
    "pd_colorbar = plt.cm.ScalarMappable(cmap=custom_cmap, norm=plt.Normalize(vmin=0, vmax=2046.6))\n",
    "cbar = plt.colorbar(pd_colorbar, shrink = .38, pad=.001, label = 'Population Density (person/km^2)', location=\"bottom\")\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10, bbox_to_anchor=(.95,.2), title=\"Pharmacy\")\n",
    "\n",
    "pharmacies_map[pharmacies_map['status'] == 'open'].plot(ax=ax, marker='o', facecolor='white', edgecolor='black', markersize=50, label='Pharmacy')\n",
    "pharmacies_map[pharmacies_map['status'] == 'closed'].plot(ax=ax, marker='o', facecolor='red', edgecolor='black', markersize=50, label='Pharmacy (Closed)')\n",
    "pharmacies_map[pharmacies_map['status'] == 'opened'].plot(ax=ax, marker='o', facecolor='greenyellow', edgecolor='black', markersize=50, label='Pharmacy (New)')\n",
    "\n",
    "plt.text(0.22, 0.71, 'Burlington', transform=ax.transAxes, size=11, weight=\"bold\", color='white',\n",
    "         path_effects=[path_effects.Stroke(linewidth=2, foreground='black'), path_effects.Normal()])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Export figure to results/figures\n",
    "if figsave:\n",
    "    fig1.savefig(make_fig_file(1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the Road Network\n",
    "\n",
    "Load a processed road network.  \n",
    "Then, create a geodataframe of its nodes and construct a point geography for each node from its x and y coordinates.  \n",
    "Finally, drop four nodes at the end of one-way roads, causing routing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_private_folder = \"./data/derived/private\"\n",
    "roads_processsed_path = derived_private_folder + \"/roads_processed.graphml\"\n",
    "roads_zip_file = \"./data/derived/public/roads_processed.zip\"\n",
    "\n",
    "# Unzip the processed network graph\n",
    "if not os.path.exists(roads_processsed_path) and os.path.exists(roads_zip_file):\n",
    "    print(\"Unzipping road network from\", roads_zip_file, \"Please wait...\", flush=True)\n",
    "    with zipfile.ZipFile(roads_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(derived_private_folder)\n",
    "\n",
    "# load the processed network graph\n",
    "if os.path.exists(roads_processsed_path):\n",
    "    print(\"Loading road network from\", roads_processsed_path, \"Please wait...\", flush=True)\n",
    "    G = ox.load_graphml(roads_processsed_path) \n",
    "    print(\"Data loaded.\") \n",
    "    \n",
    "else:\n",
    "    print(\"Error: could not load the road network from file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = ox.graph_to_gdfs(G, nodes=True, edges=False)\n",
    "\n",
    "# Create point geometries for each node in the graph, to make constructing catchment area polygons easier\n",
    "for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])\n",
    "\n",
    "# Drop four dead-end nodes for parking lot entrances\n",
    "nodes.drop([205007938, 5976921845, 204567135, 61755372], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the nearest node to each pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_osmid = gpd.GeoDataFrame(nodes[[\"geometry\"]]).reset_index().set_crs(epsg=4326, inplace=True)\n",
    "# print(nodes_osmid.head())\n",
    "\n",
    "pharmacies_osm = gpd.sjoin_nearest(pharmacies_df, nodes_osmid, distance_col=\"distances\")\n",
    "\n",
    "#rename column from osmid to nearest_osm, so that it works with other code\n",
    "pharmacies_osm = pharmacies_osm.rename(columns={\"osmid\": \"nearest_osm\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate catchment areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize catchment list, composed of 3 empty geodataframes for 3 distance bands\n",
    "catchments = []\n",
    "for distance in distances:\n",
    "    catchments.append(gpd.GeoDataFrame())\n",
    "    \n",
    "# initialize results geodataframe to store final catchment areas\n",
    "results = gpd.GeoDataFrame(columns = [\"geometry\",\"pharmid\",\"weight\"], crs = \"EPSG:4326\", geometry = \"geometry\")\n",
    "\n",
    "rweights = weights[::-1]\n",
    "rdistances = distances[::-1]\n",
    "print(rweights)\n",
    "print(rdistances)\n",
    "\n",
    "# calculate catchment areas\n",
    "print(\"Working on pharmacy:\")\n",
    "for ind in pharmacies_osm.index:\n",
    "    print(str(ind + 1) + \"/\" + str(len(pharmacies_osm)), end = '\\r')\n",
    "    \n",
    "    # create dictionary of nearest nodes\n",
    "    nearest_nodes = nx.single_source_dijkstra_path_length(G, pharmacies_osm['nearest_osm'][ind], rdistances[0], \"travel_time\") \n",
    "    # creating the largest graph from which shorter drive times can be extracted\n",
    "\n",
    "    # convert to a dataframe and join to nodes on the index (OSMID) to combine geographies with time (z)\n",
    "    nearest_points = pd.DataFrame.from_dict(nearest_nodes, orient='index').rename(columns = {0: 'z' } )\n",
    "    points = pd.merge(nodes, nearest_points, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    polygons = []\n",
    "    for d in range(len(rdistances)):\n",
    "        points = points.query(\"z <= \" + str(rdistances[d]))\n",
    "        convex_hull = gpd.GeoDataFrame(gpd.GeoSeries(points.unary_union.convex_hull))\n",
    "        convex_hull = convex_hull.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "        convex_hull[\"weight\"] = rweights[d]\n",
    "        polygons.append(convex_hull)\n",
    "        if d > 0:\n",
    "            polygons[d-1] = gpd.overlay(polygons[d-1], polygons[d], how = \"difference\")\n",
    "    \n",
    "    polygonsgdf = pd.concat(polygons)\n",
    "    polygonsgdf.set_crs(crs=\"EPSG:4326\", inplace=True)    \n",
    "    polygonsgdf[\"pharmid\"] = pharmacies_osm['pharmid'][ind]\n",
    "    results = pd.concat([results, polygonsgdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pharmacies_osm), \"pharmacies and\", len(results), \"polygons for\", len(results) / len(pharmacies_osm), \"polygons per pharmacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_results_file = \"./data/derived/public/catchments.gpkg\"\n",
    "# set to True to save the catchments file\n",
    "if False:\n",
    "    results.to_file(catchment_results_file)\n",
    "    \n",
    "# set to True to load the catchments file\n",
    "if False:\n",
    "    results = gpd.read_file(catchment_results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The interactive map below shows the three catchment areas for each pharmacy in our study area.\n",
    "We check for completeness by counting the catchment areas and dividing by the total number of pharmacies, expecting a value of 3 catchment areas per pharmacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(distances) < 5):\n",
    "    results.explore(\"weight\", categorical=True, cmap='GnBu', style_kwds={'fillOpacity': 0.2})\n",
    "# try results.explore(\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service-to-population ratio calculations\n",
    "To begin the process of calulating service-to-population ratios, town areas must be calculated since this is the final geographic unit we want to display our results in.\n",
    "This code calculates the areas where catchment areas overlap with different town geometries (fragments), and then estimates the population within these fragments by assessing the proportion of each town covered by a specific fragment.\n",
    "Next, these fragments are assigned weights based on the distance to a pharmacy location, culminating in an estimated population that may use services in a given pharmacy location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject catchment results to VT State Plane\n",
    "results.to_crs(\"EPSG:6589\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate population areas and overlay with catchments\n",
    "# (fragments are where each individual catchment area overlaps with a different population area)\n",
    "if geog_level == 'block':\n",
    "    blocks_df['src_area'] = blocks_df.geometry.area\n",
    "    fragments = gpd.overlay(blocks_df, results, how = 'intersection')\n",
    "else:\n",
    "    pop_df['src_area'] = pop_df.geometry.area\n",
    "    fragments = gpd.overlay(pop_df, results, how = 'intersection')\n",
    "\n",
    "# Calculate fragment areas\n",
    "fragments['frag_area'] = fragments.geometry.area\n",
    "\n",
    "# Calculate area ratios to see how much of a town is covered by the given fragment --> used to estimate the population in that fragment\n",
    "fragments['area_ratio']= fragments['frag_area'] / fragments['src_area']\n",
    "\n",
    "# Calculate fragment value by multiplying area_ratio by distance weight\n",
    "fragments['value'] = fragments['weight'] * fragments['area_ratio']\n",
    "\n",
    "# Calculate population served in each fragment\n",
    "fragments['pop_value'] = fragments[demand_population]*fragments['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a value for the total population served by each pharmacy is calculated by aggregating all the fragments within a given pharmacy catchment. Service-to-population ratios will ultimately be aggregated into Vermont municipalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum population served per pharmacy \n",
    "sum_values = fragments[[\"pharmid\",\"pop_value\"]]\n",
    "sum_values = sum_values.groupby(by = ['pharmid']).sum('pop_value')\n",
    "sum_values.rename(columns={'pop_value': 'total_value'}, inplace = True) \n",
    "sum_values.head()\n",
    "\n",
    "# Join summed values by pharmacy to pharmacies_df\n",
    "pharm_pop = sum_values.merge(pharmacies_df, on='pharmid', how = 'left')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# pop_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the pharmacy data for the chosen temporal extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_join = pharm_pop.copy().drop(columns=['geometry'])\n",
    "if temporal_extent == \"2023\":\n",
    "    pop_join = pop_join.query(\"status != 'opened'\")\n",
    "elif temporal_extent == \"2025\":\n",
    "    pop_join = pop_join.query(\"status != 'closed'\")\n",
    "elif temporal_extent == \"closed\":\n",
    "    pop_join = pop_join.query(\"status == 'closed'\")\n",
    "print(len(pop_join), \"pharmacy locations for\", temporal_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the service level for each pharmacy for the chosen full-time equivalent of pharmacy technicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of staff that work at each pharmacy\n",
    "if staffing_src == \"confidential\":\n",
    "    pop_join['week_staff'] = pop_join['week_pharm'] + techFTE * pop_join['week_tech']\n",
    "    pop_join['sat_staff'] = pop_join['sat_pharm'] + techFTE * pop_join['sat_tech']\n",
    "    pop_join['sun_staff'] = pop_join['sun_pharm'] + techFTE * pop_join['sun_tech']\n",
    "else:\n",
    "    pop_join['week_staff'] = 1 + (pop_join['week_sim_staff'] - 1) * 2 * techFTE \n",
    "    pop_join['sat_staff'] = 1 + (pop_join['sat_sim_staff'] - 1) * 2 * techFTE \n",
    "    pop_join['sun_staff'] = 1 + (pop_join['sun_sim_staff'] - 1) * 2 * techFTE \n",
    "    print(\"Caution: Simulated pharmacy data produced overall staffing levels.\")\n",
    "    print(\"For the purpose of working backward to simulate varied pharmacy technician values, we assume one pharmacist FTE per location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Service to Population Ratios for Service Points\n",
    "\n",
    "Service-to-population ratios can be calculated for each pharmacy by dividing this service value by the population value (total_value) for a pharmacy.\n",
    "An individual service-to-population ratio is derived for each hour on weekdays, Saturdays, and Sundays, as well as these days generally.\n",
    "The ratio for entire days represents a point in the day when all pharmacies are open, between the time when the last pharmacy opens and the first pharmacy closes (all pharmacists and technicians are working). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate service to pop ratio for each pharmacy for Weekdays, Saturdays, and Sundays between 7am-11pm\n",
    "def calculate_sp_ratios(pop_join, day_type):\n",
    "    pop_join[f'sp_ratio_{day_type}'] = pop_join[f'{day_type}_staff'] / pop_join['total_value']\n",
    "    \n",
    "    for hour in range(7, 24):\n",
    "        pop_join[f'sp_ratio_{hour}{day_type}'] = np.where((pop_join[f'{day_type}_open'] <= hour) & (pop_join[f'{day_type}_close'] >= hour), \n",
    "                                                       pop_join[f'{day_type}_staff'] / pop_join['total_value'], \n",
    "                                                       np.nan)\n",
    "# Weekday\n",
    "calculate_sp_ratios(pop_join, 'week')\n",
    "\n",
    "# Saturday\n",
    "calculate_sp_ratios(pop_join, 'sat')\n",
    "\n",
    "# Sunday\n",
    "calculate_sp_ratios(pop_join, 'sun')\n",
    "\n",
    "# Only display this data for testing. It reveals confidential information at the pharmacy level.\n",
    "# pop_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Local Service Accessibility\n",
    "\n",
    "Since our desired final result will be represented in county subdivisions, we do not need to recalculate network distances and catchments.\n",
    "Instead, we can join our service to population ratio data back to the fragments used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to the fragments df\n",
    "frag_join = fragments.merge(pop_join, on = 'pharmid', how = 'inner')\n",
    "\n",
    "# Only display this data for testing. It reveals confidential information at the pharmacy level.\n",
    "# frag_join.head()\n",
    "print(len(frag_join), \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the service-to-population ratios for each fragment must be multiplied by the area-weight and distance weights once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted service to population ratio for each slice of time (weight correspond to distance from the pharmacy still)\n",
    "frag_join['sp_weighted_w'] =  frag_join['sp_ratio_week'] * frag_join['value'] \n",
    "frag_join['sp_weighted_s'] =  frag_join['sp_ratio_sat'] * frag_join['value']  \n",
    "frag_join['sp_weighted_su'] =  frag_join['sp_ratio_sun'] * frag_join['value'] \n",
    "\n",
    "def calculate_weighted_sp_ratios(frag_join, day_type):\n",
    "    for hour in range(7, 24):\n",
    "        frag_join[f'sp_weighted_{hour}{day_type}'] = frag_join[f'sp_ratio_{hour}{day_type}'] * frag_join['value']\n",
    "\n",
    "# Weekday\n",
    "calculate_weighted_sp_ratios(frag_join, 'week')\n",
    "\n",
    "# Saturday\n",
    "calculate_weighted_sp_ratios(frag_join, 'sat')\n",
    "\n",
    "# Sundayfrag_join\n",
    "calculate_weighted_sp_ratios(frag_join, 'sun')\n",
    "\n",
    "# Make all NaN values 0, since these values represent 0 access\n",
    "frag_join.fillna(0, inplace = True)\n",
    "\n",
    "# frag_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, all of these ratios for the fragments are summed within a county subdivision (GEOID) to create an accessibility measure for each town and each hour of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum weighted service to population ratio by town\n",
    "# use sdGEOID if geographic data was from blocks\n",
    "# sdGEOID is the county subdivision GEOID joined by intersecting the block's point_on_surface\n",
    "if geog_level == 'block':\n",
    "    # Select ID and weighted service to population levels from fragments\n",
    "    accessibility_blocks = frag_join.loc[:, ['sdGEOID', 'GEOID']].join(frag_join.loc[:,'sp_weighted_w':])\n",
    "    # Sum weighted service to population ratio by block\n",
    "    accessibility_blocks = accessibility_blocks.groupby('GEOID').sum()\n",
    "    # Join county subdivision sdGEOID and block population by block GEOID\n",
    "    accessibility_blocks = accessibility_blocks.merge(blocks_df[['GEOID','sdGEOID',demand_population]], on='GEOID', how='inner')\n",
    "\n",
    "    # Find population-weighted average accessibility score by town\n",
    "    const_columns = ['GEOID', 'sdGEOID', demand_population]\n",
    "    popweighted_acc = accessibility_blocks.drop(columns=const_columns).mul(accessibility_blocks[demand_population], axis=0)\n",
    "    accessibility_blocks = accessibility_blocks[const_columns].join(popweighted_acc)\n",
    "    #    df_scaled = df.drop(columns='Multiplier').multiply(df['Multiplier'], axis=0)\n",
    "    \n",
    "    # replace block GEOID with county subdivision GEOID\n",
    "    accessibility_blocks = accessibility_blocks.drop(['GEOID',demand_population], axis = 1).rename(columns={'sdGEOID': 'GEOID'})\n",
    "    \n",
    "    # group by county subdivision and calculate population-weighted average accessibility\n",
    "    accessibility_towns = accessibility_blocks.groupby('GEOID').sum().reset_index()\n",
    "    const_columns = ['GEOID', demand_population]\n",
    "    accessibility_towns = accessibility_towns.merge(pop_df[const_columns], on='GEOID', how=\"left\")\n",
    "    popweighted_acc = accessibility_towns.drop(columns=const_columns).div(accessibility_towns[demand_population], axis=0)\n",
    "    accessibility_towns = accessibility_towns[const_columns].join(popweighted_acc)\n",
    "\n",
    "else:\n",
    "    # Select ID and weighted service to population levels from fragments\n",
    "    accessibility_towns = frag_join[['GEOID']].join(frag_join.loc[:,'sp_weighted_w':])\n",
    "    # Sum weighted service to population ratio by town\n",
    "    accessibility_towns = accessibility_towns.groupby('GEOID').sum().reset_index()\n",
    "    \n",
    "# accessibility_towns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessibility measures are rescaled to an estimate of pharmacist-equivalent staff (one pharmacist or two pharmacy tecnicians) per 10,000 people so that they are interpretable numbers comparable to regional statistics and targets for service to population ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale (multiply by large constant)\n",
    "#Full Days\n",
    "accessibility_towns['access_w'] = accessibility_towns['sp_weighted_w']*10000\n",
    "accessibility_towns['access_s'] = accessibility_towns['sp_weighted_s']*10000\n",
    "accessibility_towns['access_su'] = accessibility_towns['sp_weighted_su']*10000\n",
    "\n",
    "def calculate_accessibility_times(accessibility_towns, day_type):\n",
    "    for hour in range(7, 24):\n",
    "        accessibility_towns[f'access_{hour}{day_type}'] = accessibility_towns[f'sp_weighted_{hour}{day_type}'] * 10000\n",
    "\n",
    "# Weekday times\n",
    "calculate_accessibility_times(accessibility_towns, 'week')\n",
    "\n",
    "# Saturday times\n",
    "calculate_accessibility_times(accessibility_towns, 'sat')\n",
    "\n",
    "# Sunday times\n",
    "calculate_accessibility_times(accessibility_towns, 'sun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the results to geographic data and save the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"./data/derived/public/result_sets/results_\" + setID + \".gpkg\"\n",
    "\n",
    "# Select access columns from results\n",
    "accessibility_map = accessibility_towns[['GEOID']].join(accessibility_towns[[col for col in accessibility_towns.columns if col.startswith('access')]])\n",
    "\n",
    "# Select Vermont from all towns\n",
    "vt_df = pop_df.loc[pop_df['GEOID'].str.startswith('50'), ['GEOID', 'NAME', 'necta', 'total_pop', 'elderly_pop', 'minority_pop', 'pop_density', 'geometry']]\n",
    "\n",
    "# Join access results to Vermont towns\n",
    "accessibility_map = vt_df.merge(accessibility_map, on=\"GEOID\", how=\"inner\")\n",
    "                      \n",
    "if True:\n",
    "    accessibility_map.to_file(result_path)\n",
    "    \n",
    "# accessibility_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Results**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hypothesis 1 - Spatial Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekday accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up mapping_df for results section.\n",
    "# this code could be replaced with a function to load other results.\n",
    "mapping_df = accessibility_map.copy()\n",
    "# Map accessibility by Vermont town.\n",
    "mapping_df.explore(\"access_w\", tooltip=[\"access_w\", \"access_s\", \"access_su\"], cmap=\"Greens\", scheme='Quantiles', k=6, legend_kwds={'scale':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2a.** Spatial accessibility during conventional weekday business hours, representing a time period when all pharmacies are operational (maximum accessibility). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean access by NECTA type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekday accessibility by classification table\n",
    "means_by_metro = mapping_df.groupby('necta').mean()\n",
    "weekdaymean_by_metro = means_by_metro[['access_w']]\n",
    "\n",
    "weekdaymean_by_metro.columns = ['Mean Access']\n",
    "table_1 = tabulate(weekdaymean_by_metro, headers='keys', tablefmt='simple_grid')\n",
    "print(table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normal distribution in weekday metro, micro, rural\n",
    "access_w_metro = mapping_df[mapping_df['necta'] == 'Metropolitan']['access_w']\n",
    "access_w_micro = mapping_df[mapping_df['necta'] == 'Micropolitan']['access_w']\n",
    "access_w_rural = mapping_df[mapping_df['necta'] == 'Rural']['access_w']\n",
    "\n",
    "plt.hist(access_w_rural, bins=10)\n",
    "plt.hist(access_w_metro, bins=10)\n",
    "plt.hist(access_w_micro, bins=10)\n",
    "print(\"Not normal distribution. Cannot use ANOVA test. Use Kruskal-Wallis instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kruskal Wallis Test for significant difference of means between necta classification during conventional business hours\n",
    "h_statistic_1, p_value_1 = kruskal(access_w_metro, access_w_micro, access_w_rural)\n",
    "                 \n",
    "print(\"Kruskal-Wallis H Statistic:\", h_statistic_1)\n",
    "print(\"P-value:\", p_value_1)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value_1 < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in mean access during conventional weekday \\nbusiness hours between metropolitan, micropolitan, and rural towns.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in mean access between groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dunn_data = np.concatenate([access_w_metro, access_w_micro, access_w_rural])\n",
    "dunn_groups = ['access_w_metro']*len(access_w_metro) + ['access_w_micro']*len(access_w_micro) + ['access_w_rural']*len(access_w_rural)\n",
    "dunn_df = pd.DataFrame({'value': dunn_data, 'group': dunn_groups})\n",
    "\n",
    "# Perform Dunn's test with p-value adjustment (e.g., 'holm')\n",
    "dunn_results = sp.posthoc_dunn(dunn_df, val_col='value', group_col='group', p_adjust='holm')\n",
    "print(\"\\nDunn's Post Hoc Test Results (p-values):\\n\", dunn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "nectas = {'Rural': '#ccebc5',\n",
    "          'Micropolitan': '#7bccc4', \n",
    "          'Metropolitan': '#0868ac'\n",
    "          }\n",
    "\n",
    "color_list = [nectas[group] for group in mapping_df['necta']]\n",
    "\n",
    "legend_handles = []\n",
    "for key, value in nectas.items():\n",
    "    patch = mpatches.Patch(color=value, label=key)\n",
    "    legend_handles.append(patch)\n",
    "\n",
    "mapping_df.plot.scatter('pop_density', 'access_w', c=color_list, alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Population Density\")\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"Weekday Access\")\n",
    "plt.legend(handles=legend_handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot of Population and Weekday Access to illustrate relationship between NECTA classification, population density of county subdivisions, and spatial accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hypothesis 2 - Temporal Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean access by type of day, and then map accessibility by county subdivision for each type of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accessibility by day table\n",
    "mean_access_day = mapping_df[['access_w', 'access_s', 'access_su']].mean()\n",
    "mean_access_day_df = mean_access_day.to_frame().rename(columns={0: 'Mean Access'})\n",
    "mean_access_day_df.index = ['Weekday', 'Saturday', 'Sunday']\n",
    "table_2 = tabulate(mean_access_day_df, headers='keys', tablefmt='simple_grid')\n",
    "print(table_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessibility variation by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map accessibility by day of the week\n",
    "mapping_df1 = mapping_df\n",
    "maxacc = mapping_df1[['access_w', 'access_s', 'access_su']].max().max()\n",
    "#print(mapping_df1[['access_w', 'access_s', 'access_su']].min().min())  # Min accessibility value\n",
    "#print(mapping_df1[['access_w', 'access_s', 'access_su']].max().max())  # Max accessibility value\n",
    "\n",
    "fig2, axs = plt.subplots(1, 3, figsize=(22.5, 10), facecolor = 'white')\n",
    "plt.subplots_adjust(wspace=-.4)\n",
    "\n",
    "for i, column in enumerate(['access_w', 'access_s', 'access_su']):\n",
    "    ax = axs[i]\n",
    "    mapping_df1.plot(column=column, cmap='Greens', linewidth=0.2, ax=ax, edgecolor='0.8', legend=False,\n",
    "                vmin=0, vmax=maxacc)\n",
    "    mapping_df1.dissolve().boundary.plot(ax=ax, color='black', linewidth=1)\n",
    "    \n",
    "    # Plot only the merged exterior boundaries of the 'Metropolitan NECTA' group\n",
    "    metropolitan_boundary.plot(ax=ax, color='black', linewidth=.9)\n",
    "\n",
    "    # Plot only the merged exterior boundaries of the 'Micropolitan NECTA' group\n",
    "    micropolitan_boundary.plot(ax=ax, color='black', linestyle = 'dashed', linewidth=.9)\n",
    "    \n",
    "    axs[0].set_title('a) Weekday', fontsize=14)\n",
    "    axs[1].set_title('b) Saturday', fontsize=14)\n",
    "    axs[2].set_title('c) Sunday', fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin=0, vmax=maxacc), cmap='Greens'), ax=axs, #Max was set to 20 for visualization purposes\n",
    "                    orientation='horizontal', pad=.07, shrink=.5)\n",
    "cbar.set_label('Accessibility')\n",
    "\n",
    "#fig.patch.set_edgecolor('black') # Figure Border\n",
    "#fig.patch.set_linewidth(2)       # Figure Border\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "#cbar.ax.set_position([0.26, 0.05, .5, 0.05])\n",
    "cbar.ax.set_position([0.45, 0.05, .2, 0.05])\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=12, bbox_to_anchor=(.9,-.129))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Save Figure\n",
    "if figsave:\n",
    "    fig2.savefig(make_fig_file(2), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2**. Spatial accessibility across days of the week. Each map represents the maximum accessibility on each day when all pharmacies that plan to open that day are operational. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessibility by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linegraph_df = mapping_df.copy()\n",
    "\n",
    "# Define time range and labels\n",
    "hours = list(range(7, 23))  # 7 to 22 (10pm)\n",
    "time_labels = [f\"{h%12 or 12}{'am' if h < 12 else 'pm'}\" for h in hours]\n",
    "\n",
    "# Define suffixes for each day type\n",
    "day_types = {\n",
    "    'Weekday': 'week',\n",
    "    'Saturday': 'sat',\n",
    "    'Sunday': 'sun'\n",
    "}\n",
    "\n",
    "# Generate column names and calculate means\n",
    "mean_values = {}\n",
    "for day, suffix in day_types.items():\n",
    "    access_columns = [f'access_{h}{suffix}' for h in hours]\n",
    "    mean_values[day] = linegraph_df[access_columns].mean()\n",
    "\n",
    "# Plot\n",
    "fig3, ax = plt.subplots(figsize=(10, 6))\n",
    "linestyles = {'Weekday': '-', 'Saturday': '-.', 'Sunday': ':'}\n",
    "\n",
    "for day, values in mean_values.items():\n",
    "    plt.plot(time_labels, values, label=day, color='black', linestyle=linestyles[day])\n",
    "\n",
    "# Optional labels and legend\n",
    "plt.ylabel('Mean Accessibility')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "if figsave:\n",
    "    fig3.savefig(make_fig_file(3), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.** Mean accessibility throughout hours of the day, broken down by days of the week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessibility at extreme hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots(figsize=(10, 10), facecolor = \"white\")\n",
    "mapping_df.plot(column='access_23week', cmap='Greens', ax=ax, linewidth=0.2, edgecolor= '.8', vmax=.5)\n",
    "mapping_df.dissolve().boundary.plot(ax=ax, color='black', linewidth=1)\n",
    "\n",
    "cb = plt.cm.ScalarMappable(cmap='Greens', norm=plt.Normalize(vmin=0, vmax=.5))\n",
    "#cbar = plt.colorbar(cb, shrink = .5, orientation = 'horizontal', pad=0, label = 'Accessibility')\n",
    "\n",
    "metropolitan_boundary.plot(ax=ax, color='black', linewidth=.8)\n",
    "micropolitan_boundary.plot(ax=ax, color='black',linestyle = \"dashed\", linewidth=.8)\n",
    "\n",
    "#fig.patch.set_edgecolor('black') #Border\n",
    "#fig.patch.set_linewidth(2)  \n",
    "plt.subplots_adjust(right = .5, top = .9)\n",
    "\n",
    "# Colorbar and Legend\n",
    "cbar = plt.colorbar(cb, shrink = .5, orientation = 'horizontal', pad=0, label = 'Accessibility')\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=12, bbox_to_anchor=(.92,.135))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Figure\n",
    "if figsave:\n",
    "    fig4.savefig(make_fig_file(4), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 5.** Late night accessibility. Represents access between 10 pm and 7 am. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_w_test = pd.Series(mapping_df['access_w'])\n",
    "access_s_test = pd.Series(mapping_df['access_s'])\n",
    "access_su_test = pd.Series(mapping_df['access_su'])\n",
    "\n",
    "# Check for distribution between days of week. \n",
    "plt.hist(access_w_test, bins=10) \n",
    "plt.hist(access_s_test , bins=10) \n",
    "plt.hist(access_su_test, bins=10) \n",
    "print(\"Distribution is not normal. Cannot use ANOVA test. Use Kruskal-Wallis instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Kruskal-Wallis Test\n",
    "h_statistic_2, p_value_2 = kruskal(access_w_test, access_s_test, access_su_test)\n",
    "                 \n",
    "print(\"Kruskal-Wallis H Statistic:\", h_statistic_2)\n",
    "print(\"P-value:\", p_value_2)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value_2 < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in mean access between weekdays, Saturdays, and Sundays.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in mean access between days.\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dunn_data = np.concatenate([access_w_test, access_s_test, access_su_test])\n",
    "dunn_groups = ['access_w_test']*len(access_w_test) + ['access_s_test']*len(access_s_test) + ['access_su_test']*len(access_su_test)\n",
    "dunn_df = pd.DataFrame({'value': dunn_data, 'group': dunn_groups})\n",
    "\n",
    "# Perform Dunn's test with p-value adjustment (e.g., 'holm')\n",
    "dunn_results = sp.posthoc_dunn(dunn_df, val_col='value', group_col='group', p_adjust='holm')\n",
    "print(\"\\nDunn's Post Hoc Test Results (p-values):\\n\", dunn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 3 - Spatio-Temporal Dynamics\n",
    "Calculate mean access by NECTA classification and type of day. \n",
    "Then calculate the percentage change from urban to rural for each type of day by finding the normalized percent difference with:  \n",
    "`(Metro - Micro Access) / (Metro + Micro) * 100`  \n",
    "`(Metro - Rural Access) / (Metro + Rural) * 100`  \n",
    "`(Micro - Rural Access) / (Micro + Rural) * 100`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessibility by Day and metropolitan/micropolitan Table\n",
    "means_by_metro = mapping_df.groupby('necta').mean()[['access_w','access_s','access_su']]\n",
    "means_by_metro = means_by_metro.rename(columns={\"access_w\": \"Weekday\", \n",
    "                                                \"access_s\": \"Saturday\",\n",
    "                                                \"access_su\": \"Sunday\"})\n",
    "means_by_metro = means_by_metro.transpose()\n",
    "means_by_metro = means_by_metro.rename(columns={\"Metropolitan\": \"Metro\", \n",
    "                                                \"Micropolitan\": \"Micro\"})\n",
    "\n",
    "MetroN = necta_summary.at['Metropolitan', 'N']\n",
    "MicroN = necta_summary.at['Micropolitan', 'N']\n",
    "\n",
    "def pctdiff(df, col1, col2):\n",
    "    newcoldif = col1 + \"_\" + col2 + \"_dif\"\n",
    "    newcolpct = col1 + \"_\" + col2 + \"_pct\"\n",
    "    df[newcoldif] = df[col1] - df[col2]\n",
    "    df[newcolpct] = df[newcoldif]/(df[col1] + df[col2]) * 100\n",
    "    return df\n",
    "\n",
    "means_by_metro = round(pctdiff(means_by_metro, \"Metro\", \"Micro\"), 2)\n",
    "means_by_metro = round(pctdiff(means_by_metro, \"Metro\", \"Rural\"), 2)\n",
    "means_by_metro = round(pctdiff(means_by_metro, \"Micro\", \"Rural\"), 2)\n",
    "\n",
    "means_by_metro\n",
    "\n",
    "# print(tabulate(means_by_metro, tablefmt = 'fancy_grid', headers=[\"\",\"N\",\"Weekday Mean Access\", \"Saturday Mean Access\", \"Sunday Mean Access\"]))\n",
    "#means_by_metro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1.** Mean Accessibility on weekdays, Saturdays, and Sundays, broken down by non-rural and rural subdivisions. Percent differences are calculated to see how accessibility gaps differ throughout the week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of day and NECTA classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ticks = list(range(len(hours)))\n",
    "metro_categories = ['Metropolitan', 'Micropolitan', 'Rural']\n",
    "metro_colors = ['#1f78b4', '#a6cee3', '#b2df8a']\n",
    "metro_styles = [':', '-.', '-']  # One per category\n",
    "\n",
    "# Create linegraph dataframes for each day\n",
    "linegraph_dfs = {}\n",
    "for day, suffix in day_types.items():\n",
    "    columns = [f'access_{h}{suffix}' for h in hours]\n",
    "    grouped = mapping_df.groupby('necta')[columns].mean().T\n",
    "    linegraph_dfs[day] = grouped\n",
    "\n",
    "# Plotting\n",
    "fig5, axs = plt.subplots(3, 1, figsize=(10, 12), sharey=True)\n",
    "\n",
    "for i, (day, df) in enumerate(linegraph_dfs.items()):\n",
    "    ax = axs[i]\n",
    "    for j, category in enumerate(df.columns):\n",
    "        ax.plot(time_labels, df[category],\n",
    "                label=metro_categories[j] if i == 0 else \"\",  # Legend only once\n",
    "                color=metro_colors[j],\n",
    "                linestyle=metro_styles[j],\n",
    "                linewidth=2)\n",
    "    ax.set_title(day)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Mean Access Value')\n",
    "    ax.set_xticks(custom_ticks)\n",
    "    ax.set_xticklabels(time_labels, rotation=45)\n",
    "    ax.grid(axis='y', linestyle='-', linewidth=0.08, color='black')\n",
    "\n",
    "# Add legend only once\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig5.legend(handles, metro_categories, loc='lower center',\n",
    "            ncol=3, fontsize=12, bbox_to_anchor=(0.5, -0.025))\n",
    "\n",
    "# Final layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "if figsave:\n",
    "    fig5.savefig(make_fig_file(5), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 5.** Mean Accessibility across hours of the day for weekdays, Saturdays, and Sundays by NECTA classification. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3-0.9.0",
   "language": "python",
   "name": "python3-0.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
